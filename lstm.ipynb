{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-12T03:40:30.621076Z",
     "iopub.status.busy": "2025-10-12T03:40:30.620383Z",
     "iopub.status.idle": "2025-10-12T03:40:30.659486Z",
     "shell.execute_reply": "2025-10-12T03:40:30.658915Z",
     "shell.execute_reply.started": "2025-10-12T03:40:30.621050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 23692\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Đọc vocab\n",
    "# with open(\"/kaggle/input/data-lstm/vocab3.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "with open(\"/kaggle/input/data-small/vocab2.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "idx2word = {i: w for i, w in enumerate(vocab)}\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Vocab size: {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T03:40:35.382720Z",
     "iopub.status.busy": "2025-10-12T03:40:35.382140Z",
     "iopub.status.idle": "2025-10-12T03:40:35.386999Z",
     "shell.execute_reply": "2025-10-12T03:40:35.386145Z",
     "shell.execute_reply.started": "2025-10-12T03:40:35.382692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_text(text, word2idx, unk_token=\"<unk>\"):\n",
    "    tokens = text.strip().split()\n",
    "    return [word2idx.get(t, word2idx[unk_token]) for t in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T03:41:01.095635Z",
     "iopub.status.busy": "2025-10-12T03:41:01.094990Z",
     "iopub.status.idle": "2025-10-12T03:41:02.178888Z",
     "shell.execute_reply": "2025-10-12T03:41:02.178200Z",
     "shell.execute_reply.started": "2025-10-12T03:41:01.095607Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng token trong train: 2,367,821\n"
     ]
    }
   ],
   "source": [
    "# Đọc dữ liệu đã tokenized\n",
    "# with open(\"/kaggle/input/data-lstm/train (3).txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "with open(\"/kaggle/input/data-small/train (2).txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train_text = f.read()\n",
    "\n",
    "train_ids = encode_text(train_text, word2idx)\n",
    "print(f\"Số lượng token trong train: {len(train_ids):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T03:41:06.305080Z",
     "iopub.status.busy": "2025-10-12T03:41:06.304808Z",
     "iopub.status.idle": "2025-10-12T03:41:09.669842Z",
     "shell.execute_reply": "2025-10-12T03:41:09.669135Z",
     "shell.execute_reply.started": "2025-10-12T03:41:06.305059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tổng số mẫu huấn luyện: 2,367,791\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, token_ids, seq_len=30):\n",
    "        self.seq_len = seq_len\n",
    "        self.data = []\n",
    "        for i in range(0, len(token_ids) - seq_len):\n",
    "            x = token_ids[i:i+seq_len]\n",
    "            y = token_ids[i+1:i+seq_len+1]\n",
    "            self.data.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.data[idx]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "# Khởi tạo dataset\n",
    "train_dataset = LSTMDataset(train_ids, seq_len=30)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, drop_last=True)\n",
    "\n",
    "print(f\"Tổng số mẫu huấn luyện: {len(train_dataset):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T03:41:45.714428Z",
     "iopub.status.busy": "2025-10-10T03:41:45.713709Z",
     "iopub.status.idle": "2025-10-10T03:41:45.721493Z",
     "shell.execute_reply": "2025-10-10T03:41:45.720884Z",
     "shell.execute_reply.started": "2025-10-10T03:41:45.714405Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def generate_text(model, seed_text, word2idx, idx2word, max_new_tokens=50, temperature=1.0):\n",
    "    model.eval()\n",
    "    input_ids = torch.tensor(\n",
    "        [word2idx.get(w, word2idx[\"<unk>\"]) for w in seed_text.split()],\n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "    hidden = None\n",
    "    generated = seed_text.split()\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        logits, hidden = model(input_ids, hidden)\n",
    "        next_token_logits = logits[0, -1, :] / temperature\n",
    "        probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "        # Lấy xác suất cao nhất\n",
    "        max_prob = probs.max().item()\n",
    "\n",
    "        # Tìm tất cả token có xác suất bằng xác suất cao nhất\n",
    "        top_indices = (probs == max_prob).nonzero(as_tuple=True)[0]\n",
    "\n",
    "        # Nếu có nhiều token cùng xác suất cao nhất → chọn ngẫu nhiên\n",
    "        if len(top_indices) > 1:\n",
    "            next_id = top_indices[torch.randint(len(top_indices), (1,))].item()\n",
    "        else:\n",
    "            next_id = top_indices.item()\n",
    "\n",
    "        next_word = idx2word[next_id]\n",
    "        generated.append(next_word)\n",
    "\n",
    "        input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "\n",
    "        if next_word in {\"</s>\", \"<end_doc>\"}:\n",
    "            break\n",
    "\n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T07:33:43.692221Z",
     "iopub.status.busy": "2025-10-10T07:33:43.691520Z",
     "iopub.status.idle": "2025-10-10T07:33:43.699829Z",
     "shell.execute_reply": "2025-10-10T07:33:43.699144Z",
     "shell.execute_reply.started": "2025-10-10T07:33:43.692196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "def generate_text_with_context(\n",
    "    model, seed_text, word2idx, idx2word,\n",
    "    max_sentences=5, max_tokens_per_sentence=40, context_window=5, device='cuda'\n",
    "):\n",
    "    model.eval()\n",
    "    generated = []\n",
    "    hidden = None\n",
    "\n",
    "    # Mã hóa seed ban đầu\n",
    "    input_ids = torch.tensor(\n",
    "        [word2idx.get(w, word2idx[\"<unk>\"]) for w in seed_text.split()],\n",
    "        dtype=torch.long\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "    current_sentence = seed_text.split()\n",
    "\n",
    "    for sent_id in range(max_sentences):\n",
    "        for _ in range(max_tokens_per_sentence):\n",
    "            logits, hidden = model(input_ids, hidden)\n",
    "            probs = F.softmax(logits[0, -1, :], dim=-1)\n",
    "\n",
    "            # Chọn token có xác suất cao nhất (nếu nhiều token có cùng xác suất → random)\n",
    "            max_prob = probs.max().item()\n",
    "            top_indices = (probs == max_prob).nonzero(as_tuple=True)[0]\n",
    "            if len(top_indices) > 1:\n",
    "                next_id = random.choice(top_indices.tolist())\n",
    "            else:\n",
    "                next_id = top_indices.item()\n",
    "\n",
    "            next_word = idx2word[next_id]\n",
    "            current_sentence.append(next_word)\n",
    "\n",
    "            input_ids = torch.tensor([[next_id]], dtype=torch.long).to(device)\n",
    "\n",
    "            if next_word in {\"</s>\", \"<end_doc>\"}:\n",
    "                break\n",
    "\n",
    "        generated.extend(current_sentence)\n",
    "\n",
    "        # Nếu gặp token kết thúc tài liệu → dừng\n",
    "        if next_word in {\"<end_doc>\"}:\n",
    "            break\n",
    "\n",
    "        # Lấy một phần ngữ cảnh (context_window từ cuối của câu vừa sinh)\n",
    "        context = current_sentence[-context_window:]\n",
    "        input_ids = torch.tensor(\n",
    "            [word2idx.get(w, word2idx[\"<unk>\"]) for w in context],\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0).to(device)\n",
    "\n",
    "        current_sentence = []  # bắt đầu câu mới, nhưng giữ hidden\n",
    "\n",
    "    return \" \".join(generated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T07:35:25.372161Z",
     "iopub.status.busy": "2025-10-10T07:35:25.371876Z",
     "iopub.status.idle": "2025-10-10T07:35:25.700475Z",
     "shell.execute_reply": "2025-10-10T07:35:25.699810Z",
     "shell.execute_reply.started": "2025-10-10T07:35:25.372141Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_doc> <s> tp hcm tăng_cường thêm xe phục_vụ thí_sinh tp hcm hôm_nay , ông <unk> , phó phòng thanh_tra sở văn_hóa thông_tin hà_nội , cho biết , theo kế_hoạch , cuối năm 2005 , thanh_tra sở văn_hóa thông_tin hà_nội đã xử_phạt hàng tỷ đồng , tịch_thu hơn 120.000 cuốn sách , hơn 15 tấn sách bán thành_phẩm các loại chưa kịp <unk> 11.000 tờ hóa_đơn tài_chính ... ngoài_ra còn có chương_trình sử_dụng dịch_vụ ăn_uống , giải_khát cao_cấp . </s> <s> ngoài_ra , khách tham_dự festival còn có_thể <unk> nháp rượu vang đà lạt và của một_số nước chuyên sản_xuất sản_phẩm trước_đây . </s> <s> các tour du_lịch được việt_kiều quan_tâm nhất là nghỉ_ngơi cùng gia_đình và khám_phá vẻ đẹp đất_nước . </s> <s> bà tô <unk> , phó giám_đốc công_ty du_lịch bến thành , cho biết thêm : \" tàu của anh vẫn còn nằm trên biển , mặc_dù không_thể leo lên đỉnh_cao hơn ngoại_trừ \" một fan \" ở đây , là một trong những người đầu_tiên lái boeing 777 hạ_cánh xuống sân_bay san_francisco , mang đến cho nhân_dân mỹ hình_ảnh của một nước việt_nam mới đang vươn lên mạnh_mẽ \" . </s> <s> đó là niềm tự_hào của anh , là niềm vui lớn , càng nhớ càng nhiều càng thêm và sáng sớm cá_độ diễn ra đêm . </s> <s> vnexpress đã có cuộc trao_đổi với ông nhân_dịp 50 năm chiến_thắng điện_biên_phủ . </s> <s> - kỷ_niệm nào đáng nhớ nhất với trung_tướng tại chiến_dịch điện_biên_phủ ? </s> <s> - tôi vinh_dự được tham_gia chiến_dịch và có rất nhiều kỷ_niệm . </s>\n"
     ]
    }
   ],
   "source": [
    "output = generate_text_with_context(\n",
    "    model, \"<start_doc> <s> tp hcm tăng_cường thêm xe phục_vụ thí_sinh\", word2idx, idx2word,\n",
    "    max_sentences=10, max_tokens_per_sentence=50, context_window=50\n",
    ")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T17:57:43.529492Z",
     "iopub.status.busy": "2025-10-09T17:57:43.529252Z",
     "iopub.status.idle": "2025-10-09T17:57:43.958739Z",
     "shell.execute_reply": "2025-10-09T17:57:43.957972Z",
     "shell.execute_reply.started": "2025-10-09T17:57:43.529475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Lưu trọng số của mô hình\n",
    "torch.save(model.state_dict(), \"/kaggle/working/model_final.pth\")\n",
    "\n",
    "print(\"Model đã được lưu tại /kaggle/working/model_final.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T07:04:47.939393Z",
     "iopub.status.busy": "2025-10-10T07:04:47.939114Z",
     "iopub.status.idle": "2025-10-10T07:04:49.099752Z",
     "shell.execute_reply": "2025-10-10T07:04:49.098983Z",
     "shell.execute_reply.started": "2025-10-10T07:04:47.939372Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint đã được lưu!\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss.item(),\n",
    "}, \"/kaggle/working/checkpoint.pth\")\n",
    "\n",
    "print(\"Checkpoint đã được lưu!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-09T18:20:55.181223Z",
     "iopub.status.busy": "2025-10-09T18:20:55.180951Z",
     "iopub.status.idle": "2025-10-09T18:20:55.866704Z",
     "shell.execute_reply": "2025-10-09T18:20:55.865920Z",
     "shell.execute_reply.started": "2025-10-09T18:20:55.181204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 1. Khởi tạo lại kiến trúc model (phải giống khi bạn train)\n",
    "model = LSTMLM(vocab_size).to(device)\n",
    "\n",
    "# 2. Load trọng số đã lưu\n",
    "model.load_state_dict(torch.load(\"model_final.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "# 3. Đặt chế độ eval\n",
    "model.eval()\n",
    "\n",
    "print(\"Mô hình đã được load thành công và sẵn sàng dùng!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T03:53:20.924073Z",
     "iopub.status.busy": "2025-10-10T03:53:20.923376Z",
     "iopub.status.idle": "2025-10-10T03:53:21.939077Z",
     "shell.execute_reply": "2025-10-10T03:53:21.938385Z",
     "shell.execute_reply.started": "2025-10-10T03:53:20.924049Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Khôi phục từ epoch 4 với loss 1.6111\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import zipfile\n",
    "\n",
    "# # Giải nén file zip vào working directory\n",
    "# with zipfile.ZipFile(\"/kaggle/input/model_final.zip\", 'r') as zip_ref:\n",
    "#     zip_ref.extractall(\"/kaggle/working\")\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Tạo lại model và optimizer\n",
    "model = LSTMLM(vocab_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"/kaggle/input/lstm/pytorch/default/1/checkpoint (8).pth\", map_location=device)\n",
    "\n",
    "# Khôi phục trạng thái mô hình và optimizer\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch']\n",
    "loss_value = checkpoint['loss']\n",
    "\n",
    "print(f\"Khôi phục từ epoch {start_epoch} với loss {loss_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-10T03:53:23.917438Z",
     "iopub.status.busy": "2025-10-10T03:53:23.916883Z",
     "iopub.status.idle": "2025-10-10T07:00:24.258207Z",
     "shell.execute_reply": "2025-10-10T07:00:24.257390Z",
     "shell.execute_reply.started": "2025-10-10T03:53:23.917415Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|█████████████████████████████████| 9205/9205 [1:33:33<00:00,  1.64it/s, loss=1.5057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 | Average Loss: 1.5057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/5: 100%|█████████████████████████████████| 9205/9205 [1:33:26<00:00,  1.64it/s, loss=1.4471]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/5 | Average Loss: 1.4471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "n_epochs = 7  # tổng số epoch muốn train đến\n",
    "for epoch in range(start_epoch, n_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # tqdm hiển thị tiến độ theo batch\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                        desc=f\"Epoch {epoch+1}/{n_epochs}\", ncols=100)\n",
    "\n",
    "    for batch_idx, (x, y) in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits, _ = model(x)\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Hiển thị loss tạm thời ngay trên thanh tqdm\n",
    "        progress_bar.set_postfix({\"loss\": f\"{total_loss/(batch_idx+1):.4f}\"})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Average Loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM mới"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T03:35:54.454279Z",
     "iopub.status.busy": "2025-10-12T03:35:54.454017Z",
     "iopub.status.idle": "2025-10-12T03:35:54.461411Z",
     "shell.execute_reply": "2025-10-12T03:35:54.460375Z",
     "shell.execute_reply.started": "2025-10-12T03:35:54.454259Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class LSTMLM2(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=512, hidden_size=1024, num_layers=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(\n",
    "            emb_dim, hidden_size, num_layers,\n",
    "            batch_first=True, dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, input_ids, hx=None):\n",
    "        emb = self.embed(input_ids)            # (B, T, E)\n",
    "        out, hx = self.lstm(emb, hx)           # (B, T, H)\n",
    "        logits = self.fc(out)                  # (B, T, V)\n",
    "        return logits, hx\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T03:42:09.979215Z",
     "iopub.status.busy": "2025-10-12T03:42:09.978623Z",
     "iopub.status.idle": "2025-10-12T08:14:18.538590Z",
     "shell.execute_reply": "2025-10-12T08:14:18.537597Z",
     "shell.execute_reply.started": "2025-10-12T03:42:09.979192Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3: 100%|█████████████████████████████████| 9249/9249 [1:30:54<00:00,  1.70it/s, loss=3.5771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 | Average Loss: 3.5771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|█████████████████████████████████| 9249/9249 [1:30:36<00:00,  1.70it/s, loss=2.5437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/3 | Average Loss: 2.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|█████████████████████████████████| 9249/9249 [1:30:37<00:00,  1.70it/s, loss=2.2673]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/3 | Average Loss: 2.2673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Khởi tạo mô hình\n",
    "model = LSTMLM2(vocab_size=vocab_size, emb_dim=512, hidden_size=1024, num_layers=2, dropout=0.3)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss và optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 3\n",
    "batch_size = train_loader.batch_size  # giả sử loader đã định nghĩa batch_size\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Khởi tạo hidden state cho batch đầu tiên\n",
    "    hidden = model.init_hidden(batch_size, device)\n",
    "\n",
    "    # Dùng tqdm để hiển thị tiến độ\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                        desc=f\"Epoch {epoch+1}/{n_epochs}\", ncols=100)\n",
    "\n",
    "    for batch_idx, (x, y) in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Đảm bảo hidden có đúng kích thước\n",
    "        if hidden[0].size(1) != x.size(0):\n",
    "            hidden = model.init_hidden(x.size(0), device)\n",
    "            \n",
    "        # Forward pass\n",
    "        logits, hidden = model(x, hidden)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Detach hidden state để không lan gradient qua toàn bộ lịch sử\n",
    "        # hidden = (hidden[0].detach(), hidden[1].detach())\n",
    "        hidden = tuple(h.detach() for h in hidden)\n",
    "\n",
    "        # Hiển thị loss tạm thời\n",
    "        progress_bar.set_postfix({\"loss\": f\"{total_loss/(batch_idx+1):.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Average Loss: {avg_loss:.4f}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T08:16:27.105638Z",
     "iopub.status.busy": "2025-10-12T08:16:27.105030Z",
     "iopub.status.idle": "2025-10-12T08:16:28.382577Z",
     "shell.execute_reply": "2025-10-12T08:16:28.381840Z",
     "shell.execute_reply.started": "2025-10-12T08:16:27.105615Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model đã được lưu vào: model_lstm_lm2.pt\n"
     ]
    }
   ],
   "source": [
    "# Lưu checkpoint sau khi train xong\n",
    "save_path = \"model_lstm_lm2.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch + 1,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': avg_loss,\n",
    "    'vocab_size': vocab_size,\n",
    "}, save_path)\n",
    "\n",
    "print(f\"Model đã được lưu vào: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:34:16.750203Z",
     "iopub.status.busy": "2025-10-12T14:34:16.749403Z",
     "iopub.status.idle": "2025-10-12T14:34:17.849815Z",
     "shell.execute_reply": "2025-10-12T14:34:17.849099Z",
     "shell.execute_reply.started": "2025-10-12T14:34:16.750177Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model đã load lại từ epoch 4 với loss 2.2673\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo lại mô hình cùng cấu hình\n",
    "loaded_model = LSTMLM2(vocab_size=vocab_size, emb_dim=512, hidden_size=1024, num_layers=2, dropout=0.3)\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "# Tạo optimizer mới\n",
    "loaded_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"/kaggle/working/model_lstm_lm2.pt\", map_location=device)\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "last_loss = checkpoint['loss']\n",
    "\n",
    "print(f\"Model đã load lại từ epoch {start_epoch} với loss {last_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T09:20:40.161422Z",
     "iopub.status.busy": "2025-10-12T09:20:40.161166Z",
     "iopub.status.idle": "2025-10-12T13:53:07.849097Z",
     "shell.execute_reply": "2025-10-12T13:53:07.848284Z",
     "shell.execute_reply.started": "2025-10-12T09:20:40.161402Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/6: 100%|█████████████████████████████████| 9249/9249 [1:30:53<00:00,  1.70it/s, loss=2.1312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/6 | Average Loss: 2.1312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/6: 100%|█████████████████████████████████| 9249/9249 [1:30:49<00:00,  1.70it/s, loss=2.0447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/6 | Average Loss: 2.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/6: 100%|█████████████████████████████████| 9249/9249 [1:30:44<00:00,  1.70it/s, loss=1.9815]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/6 | Average Loss: 1.9815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Giả sử bạn đã có:\n",
    "# loaded_model, loaded_optimizer, criterion, start_epoch, last_loss, train_loader, vocab_size, device\n",
    "\n",
    "n_epochs_total = 6\n",
    "\n",
    "for epoch in range(start_epoch, n_epochs_total + 1):\n",
    "    loaded_model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # Khởi tạo hidden state cho batch đầu tiên\n",
    "    hidden = loaded_model.init_hidden(train_loader.batch_size, device)\n",
    "\n",
    "    # Thanh tiến độ tqdm\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader),\n",
    "                        desc=f\"Epoch {epoch}/{n_epochs_total}\", ncols=100)\n",
    "\n",
    "    for batch_idx, (x, y) in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loaded_optimizer.zero_grad()\n",
    "\n",
    "        # Nếu batch cuối nhỏ hơn batch_size → reset hidden\n",
    "        if hidden[0].size(1) != x.size(0):\n",
    "            hidden = loaded_model.init_hidden(x.size(0), device)\n",
    "\n",
    "        # Forward\n",
    "        logits, hidden = loaded_model(x, hidden)\n",
    "\n",
    "        # Tính loss\n",
    "        loss = criterion(logits.view(-1, vocab_size), y.view(-1))\n",
    "        loss.backward()\n",
    "        loaded_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Ngắt kết nối gradient quá khứ (tránh exploding graph)\n",
    "        hidden = tuple(h.detach() for h in hidden)\n",
    "\n",
    "        # Hiển thị loss tạm thời\n",
    "        progress_bar.set_postfix({\"loss\": f\"{total_loss / (batch_idx + 1):.4f}\"})\n",
    "\n",
    "    # Tính loss trung bình epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch}/{n_epochs_total} | Average Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T13:53:20.059538Z",
     "iopub.status.busy": "2025-10-12T13:53:20.059004Z",
     "iopub.status.idle": "2025-10-12T13:53:21.312356Z",
     "shell.execute_reply": "2025-10-12T13:53:21.311431Z",
     "shell.execute_reply.started": "2025-10-12T13:53:20.059516Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint đã được lưu!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lưu checkpoint\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': loaded_model.state_dict(),\n",
    "    'optimizer_state_dict': loaded_optimizer.state_dict(),\n",
    "    'loss': avg_loss,\n",
    "}, \"/kaggle/working/model_lstm_lm2_cont.pt\")\n",
    "\n",
    "print(\"Checkpoint đã được lưu!\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:36:11.247648Z",
     "iopub.status.busy": "2025-10-12T14:36:11.247015Z",
     "iopub.status.idle": "2025-10-12T14:36:12.230839Z",
     "shell.execute_reply": "2025-10-12T14:36:12.230161Z",
     "shell.execute_reply.started": "2025-10-12T14:36:11.247621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model đã load lại từ epoch 7 với loss 1.9815\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo lại mô hình cùng cấu hình\n",
    "loaded_model = LSTMLM2(vocab_size=vocab_size, emb_dim=512, hidden_size=1024, num_layers=2, dropout=0.3)\n",
    "loaded_model = loaded_model.to(device)\n",
    "\n",
    "# Tạo optimizer mới\n",
    "loaded_optimizer = optim.Adam(loaded_model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(\"/kaggle/working/model_lstm_lm2_cont.pt\", map_location=device)\n",
    "\n",
    "loaded_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "loaded_optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "start_epoch = checkpoint['epoch'] + 1\n",
    "last_loss = checkpoint['loss']\n",
    "\n",
    "print(f\"Model đã load lại từ epoch {start_epoch} với loss {last_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:36:15.838083Z",
     "iopub.status.busy": "2025-10-12T14:36:15.837519Z",
     "iopub.status.idle": "2025-10-12T14:36:15.842733Z",
     "shell.execute_reply": "2025-10-12T14:36:15.841747Z",
     "shell.execute_reply.started": "2025-10-12T14:36:15.838059Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "Số lượng state lưu trong optimizer: 11\n"
     ]
    }
   ],
   "source": [
    "for param_group in loaded_optimizer.param_groups:\n",
    "    print(\"Learning rate:\", param_group['lr'])\n",
    "\n",
    "print(\"Số lượng state lưu trong optimizer:\", len(loaded_optimizer.state))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:25:59.411542Z",
     "iopub.status.busy": "2025-10-12T14:25:59.411047Z",
     "iopub.status.idle": "2025-10-12T14:25:59.420400Z",
     "shell.execute_reply": "2025-10-12T14:25:59.419709Z",
     "shell.execute_reply.started": "2025-10-12T14:25:59.411519Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_from_text(model, prompt_text, word2idx, idx2word,\n",
    "                       max_tokens_total=100, top_p=0.9, temperature=0.8,\n",
    "                       max_sentences=4, device=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Sinh văn bản từ prompt_text, giữ hidden state liên tục (phù hợp LSTM).\n",
    "    Dừng khi đạt:\n",
    "      - max_tokens_total token, hoặc\n",
    "      - <end_doc>, hoặc\n",
    "      - đủ 3–4 câu (max_sentences)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # ---- Encode: text → token IDs ----\n",
    "    start_tokens = [word2idx.get(w, word2idx.get(\"<unk>\", 0)) for w in prompt_text.split()]\n",
    "    input_ids = torch.tensor([start_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "    # Khởi tạo hidden state\n",
    "    hidden = model.init_hidden(batch_size=1, device=device)\n",
    "\n",
    "    # Chạy prompt để cập nhật hidden\n",
    "    logits, hidden = model(input_ids, hidden)\n",
    "\n",
    "    generated = start_tokens.copy()\n",
    "    sentence_count = 0\n",
    "\n",
    "    for _ in range(max_tokens_total):\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        # Top-p sampling\n",
    "        sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "        cutoff_mask = cumulative_probs > top_p\n",
    "        if cutoff_mask.any():\n",
    "            cutoff_idx = torch.where(cutoff_mask)[1][0]\n",
    "            sorted_probs = sorted_probs[:, :cutoff_idx + 1]\n",
    "            sorted_idx = sorted_idx[:, :cutoff_idx + 1]\n",
    "        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "        next_token = sorted_idx[0, torch.multinomial(sorted_probs[0], 1)].item()\n",
    "        generated.append(next_token)\n",
    "\n",
    "        # Kiểm tra điều kiện dừng\n",
    "        word = idx2word[next_token]\n",
    "        if word in {\".\", \"!\", \"?\"}:\n",
    "            sentence_count += 1\n",
    "        if word == \"<end_doc>\" or sentence_count >= max_sentences:\n",
    "            break\n",
    "\n",
    "        # Tiếp tục sinh\n",
    "        input_ids = torch.tensor([[next_token]], device=device)\n",
    "        logits, hidden = model(input_ids, hidden)\n",
    "\n",
    "    # ---- Decode ----\n",
    "    output_words = [idx2word[t] for t in generated]\n",
    "    return \" \".join(output_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:27:22.645317Z",
     "iopub.status.busy": "2025-10-12T14:27:22.645047Z",
     "iopub.status.idle": "2025-10-12T14:27:22.655844Z",
     "shell.execute_reply": "2025-10-12T14:27:22.654987Z",
     "shell.execute_reply.started": "2025-10-12T14:27:22.645298Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_from_text(model, prompt_text, word2idx, idx2word,\n",
    "                       max_tokens_total=100, top_p=0.9, temperature=0.8,\n",
    "                       max_sentences=4, device=\"cpu\", mode=\"top-p\"):\n",
    "    \"\"\"\n",
    "    Sinh văn bản từ prompt_text với chế độ:\n",
    "      - mode='top-p'  → sampling (random theo xác suất)\n",
    "      - mode='greedy' → chọn token xác suất cao nhất\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    # ---- Encode: text → token IDs ----\n",
    "    start_tokens = [word2idx.get(w, word2idx.get(\"<unk>\", 0)) for w in prompt_text.split()]\n",
    "    input_ids = torch.tensor([start_tokens], dtype=torch.long, device=device)\n",
    "\n",
    "    # Khởi tạo hidden state\n",
    "    hidden = model.init_hidden(batch_size=1, device=device)\n",
    "\n",
    "    # Chạy prompt để cập nhật hidden\n",
    "    logits, hidden = model(input_ids, hidden)\n",
    "\n",
    "    generated = start_tokens.copy()\n",
    "    sentence_count = 0\n",
    "\n",
    "    for _ in range(max_tokens_total):\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "        if mode == \"greedy\":\n",
    "            # Chọn token có xác suất cao nhất\n",
    "            next_token = torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "        elif mode == \"top-p\":\n",
    "            # Top-p sampling\n",
    "            sorted_probs, sorted_idx = torch.sort(probs, descending=True)\n",
    "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            cutoff_mask = cumulative_probs > top_p\n",
    "            if cutoff_mask.any():\n",
    "                cutoff_idx = torch.where(cutoff_mask)[1][0]\n",
    "                sorted_probs = sorted_probs[:, :cutoff_idx + 1]\n",
    "                sorted_idx = sorted_idx[:, :cutoff_idx + 1]\n",
    "            sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)\n",
    "\n",
    "            # Phải đảm bảo sampling cũng trên GPU\n",
    "            next_token = sorted_idx[0, torch.multinomial(sorted_probs[0].to(device), 1)].item()\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"mode phải là 'top-p' hoặc 'greedy'\")\n",
    "\n",
    "        generated.append(next_token)\n",
    "\n",
    "        # Kiểm tra điều kiện dừng\n",
    "        word = idx2word[next_token]\n",
    "        if word in {\".\", \"!\", \"?\"}:\n",
    "            sentence_count += 1\n",
    "        if word == \"<end_doc>\" or sentence_count >= max_sentences:\n",
    "            break\n",
    "\n",
    "        # Tiếp tục sinh token tiếp theo\n",
    "        input_ids = torch.tensor([[next_token]], device=device)\n",
    "        logits, hidden = model(input_ids, hidden)\n",
    "\n",
    "    # ---- Decode ----\n",
    "    output_words = [idx2word[t] for t in generated]\n",
    "    return \" \".join(output_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:17:34.902393Z",
     "iopub.status.busy": "2025-10-12T14:17:34.901648Z",
     "iopub.status.idle": "2025-10-12T14:17:34.905889Z",
     "shell.execute_reply": "2025-10-12T14:17:34.905101Z",
     "shell.execute_reply.started": "2025-10-12T14:17:34.902366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "prompt = \"busan lợi_thế khán_giả nhà và cả cái nóng\"\n",
    "print(generate_from_text(loaded_model, prompt, word2idx, idx2word, mode=\"greedy\"))\n",
    "print(generate_from_text(loaded_model, prompt, word2idx, idx2word, mode=\"top-p\", top_p=0.9))\n",
    "# print(generate_from_text(loaded_model, prompt, word2idx, idx2word, mode=\"top_k\", top_k=40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# prompt_text = \"<start_doc> <s> tp hcm tăng_cường thêm xe phục_vụ\"\n",
    "prompt_text = \"busan lợi_thế khán_giả nhà và cả cái nóng\"\n",
    "output = generated_text = generate_from_text(model, prompt_text, word2idx, idx2word, device=device)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:01:28.231929Z",
     "iopub.status.busy": "2025-10-12T14:01:28.231623Z",
     "iopub.status.idle": "2025-10-12T14:01:28.383971Z",
     "shell.execute_reply": "2025-10-12T14:01:28.383300Z",
     "shell.execute_reply.started": "2025-10-12T14:01:28.231908Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start_doc> <s> có_thể phải dùng phiếu để dân giám_sát . </s> <s> bởi lương_thực và các vật_dụng khác cũng có_thể khan_hiếm . </s> <s> các công_trình này đều được xây_dựng khang_trang , sạch_đẹp . </s> <s> tại đây , nhân_ngày quốc_tế thiếu_nhi 1 1 , tại trại_giam hóa lao_động vn sáng nay , cục quản_lý lao_động ngoài nước đã có buổi làm_việc với ban pháp_chế - thường_trực hđnd tp.hcm ; ông phượng cũng cho rằng , những lùm_xùm trong vụ kiện này không_chỉ vì_thế_hệ hiện_nay mà_còn vì chính sự thú_vị mà chỉ cần ăn 1 giờ là như_thế .\n"
     ]
    }
   ],
   "source": [
    "# prompt_text = \"<start_doc> <s> tp hcm tăng_cường thêm xe phục_vụ\"\n",
    "prompt_text = \"<start_doc> <s> có_thể phải dùng phiếu để\"\n",
    "output = generated_text = generate_from_text(model, prompt_text, word2idx, idx2word, device=device)\n",
    "\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:01:47.297669Z",
     "iopub.status.busy": "2025-10-12T14:01:47.297072Z",
     "iopub.status.idle": "2025-10-12T14:01:47.305065Z",
     "shell.execute_reply": "2025-10-12T14:01:47.304216Z",
     "shell.execute_reply.started": "2025-10-12T14:01:47.297641Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_perplexity(model, data_loader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    hidden = model.init_hidden(batch_size=data_loader.batch_size, device=device)\n",
    "\n",
    "    # Thanh tiến độ\n",
    "    progress_bar = tqdm(enumerate(data_loader), total=len(data_loader),\n",
    "                        desc=\"Evaluating\", ncols=100)\n",
    "\n",
    "    for batch_idx, (x, y) in progress_bar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        # Reset hidden nếu batch_size thay đổi (cuối epoch)\n",
    "        if hidden[0].size(1) != x.size(0):\n",
    "            hidden = model.init_hidden(x.size(0), device)\n",
    "\n",
    "        logits, hidden = model(x, hidden)\n",
    "\n",
    "        # Tính loss cho batch\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "\n",
    "        # Nhân với số lượng token trong batch\n",
    "        total_loss += loss.item() * x.numel()\n",
    "        total_tokens += x.numel()\n",
    "\n",
    "        # Cắt gradient để tránh tích lũy\n",
    "        hidden = tuple(h.detach() for h in hidden)\n",
    "\n",
    "        # Cập nhật tiến độ\n",
    "        avg_loss = total_loss / max(total_tokens, 1)\n",
    "        progress_bar.set_postfix({\n",
    "            \"avg_loss\": f\"{avg_loss:.4f}\",\n",
    "            \"PPL\": f\"{math.exp(min(avg_loss, 100)):.2f}\"\n",
    "        })\n",
    "\n",
    "    # Trung bình cuối cùng\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    # perplexity = math.exp(min(avg_loss, 100))  # tránh tràn số\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    print(f\"\\nAverage loss: {avg_loss:.4f} | Perplexity: {perplexity:.2f}\")\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:01:55.737832Z",
     "iopub.status.busy": "2025-10-12T14:01:55.737512Z",
     "iopub.status.idle": "2025-10-12T14:01:57.799362Z",
     "shell.execute_reply": "2025-10-12T14:01:57.798785Z",
     "shell.execute_reply.started": "2025-10-12T14:01:55.737804Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng token trong test: 292,793\n"
     ]
    }
   ],
   "source": [
    "def encode_text(text, word2idx, unk_token=\"<unk>\"):\n",
    "    tokens = text.strip().split()\n",
    "    return [word2idx.get(t, word2idx[unk_token]) for t in tokens]\n",
    "\n",
    "with open(\"/kaggle/input/data-small/test (1).txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_text = f.read()\n",
    "\n",
    "test_ids = encode_text(test_text, word2idx)\n",
    "print(f\"Số lượng token trong test: {len(test_ids):,}\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = LSTMDataset(test_ids, seq_len=30)\n",
    "test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T14:36:25.178131Z",
     "iopub.status.busy": "2025-10-12T14:36:25.177683Z",
     "iopub.status.idle": "2025-10-12T14:39:56.056762Z",
     "shell.execute_reply": "2025-10-12T14:39:56.056140Z",
     "shell.execute_reply.started": "2025-10-12T14:36:25.178107Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████| 1143/1143 [03:30<00:00,  5.42it/s, avg_loss=5.8143, PPL=335.06]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 5.8143 | Perplexity: 335.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "335.0642437640533"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "evaluate_perplexity(loaded_model, test_loader, criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T09:01:16.992023Z",
     "iopub.status.busy": "2025-10-12T09:01:16.991697Z",
     "iopub.status.idle": "2025-10-12T09:04:48.524820Z",
     "shell.execute_reply": "2025-10-12T09:04:48.524180Z",
     "shell.execute_reply.started": "2025-10-12T09:01:16.992000Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████████████| 1143/1143 [03:31<00:00,  5.40it/s, avg_loss=5.4743, PPL=238.49]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average loss: 5.4743 | Perplexity: 238.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "238.4883210939928"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "evaluate_perplexity(model, test_loader, criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-12T15:19:14.510044Z",
     "iopub.status.busy": "2025-10-12T15:19:14.509724Z",
     "iopub.status.idle": "2025-10-12T15:20:12.192696Z",
     "shell.execute_reply": "2025-10-12T15:20:12.191860Z",
     "shell.execute_reply.started": "2025-10-12T15:19:14.510023Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2367791 câu, 70296910 token\n",
      "Test: 292763 câu, 8618432 token\n"
     ]
    }
   ],
   "source": [
    "num_sent_train = len(train_loader.dataset)\n",
    "num_sent_test = len(test_loader.dataset)\n",
    "\n",
    "num_token_train = 0\n",
    "for x, y in train_loader:\n",
    "    num_token_train += (y != 0).sum().item()  # nếu có padding = 0\n",
    "\n",
    "num_token_test = 0\n",
    "for x, y in test_loader:\n",
    "    num_token_test += (y != 0).sum().item()\n",
    "\n",
    "print(f\"Train: {num_sent_train} câu, {num_token_train} token\")\n",
    "print(f\"Test: {num_sent_test} câu, {num_token_test} token\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8436950,
     "sourceId": 13309778,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8450115,
     "sourceId": 13328544,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8461542,
     "sourceId": 13343471,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 468241,
     "modelInstanceId": 451918,
     "sourceId": 602868,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 469754,
     "modelInstanceId": 453456,
     "sourceId": 604664,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
